// app/api/chat/route.js
import { NextResponse } from 'next/server';
import { getScenario, getNextNode, interpolateMessage, findActionByTrigger, getScenarioList, runScenario, getScenarioCategories } from '../../lib/chatbotEngine';
import { getLlmResponse } from '../../lib/llm';
import { locales } from '../../lib/locales';
// --- ğŸ‘‡ [ìˆ˜ì •] getErrorKey ì„í¬íŠ¸ ì œê±° ---
// import { getErrorKey } from '../../lib/errorHandler';

const actionHandlers = {
    'GET_SCENARIO_LIST': async (payload, slots, language) => {
        try {
            const scenarios = await getScenarioList();
            return NextResponse.json({
                type: 'scenario_list',
                scenarios,
                message: locales[language].scenarioListMessage || locales['en'].scenarioListMessage,
                scenarioState: null
            });
        } catch (error) {
            console.error('[ActionHandler Error] GET_SCENARIO_LIST:', error);
            // --- ğŸ‘‡ [ìˆ˜ì •] errorLLMFail ì‚¬ìš© ---
            const message = locales[language]?.['errorLLMFail'] || 'Failed to get scenario list.';
            // --- ğŸ‘† [ìˆ˜ì •] ---
            return NextResponse.json({ type: 'error', message }, { status: 500 });
        }
    },
    'START_SCENARIO': async (payload, slots, language) => {
        const { scenarioId } = payload;
        try {
            const scenario = await getScenario(scenarioId);
            const startNode = getNextNode(scenario, null, null, slots);

            if (!startNode || !startNode.data) {
                const message = `Scenario '${scenarioId}' could not be started. (Content might be empty or start node missing)`;
                console.warn(message);
                return NextResponse.json({
                    type: 'scenario_end',
                    message: message,
                    scenarioState: null,
                    slots: slots
                });
            }

            if (startNode.data.content) {
                startNode.data.content = interpolateMessage(startNode.data.content, slots);
            }

            return NextResponse.json({
                type: 'scenario_start',
                nextNode: startNode,
                scenarioState: { scenarioId: scenarioId, currentNodeId: startNode.id, awaitingInput: startNode.type === 'slotfilling' },
                slots: slots
            });
        } catch (error) {
            console.error(`[ActionHandler Error] START_SCENARIO (${scenarioId}):`, error);
            // --- ğŸ‘‡ [ìˆ˜ì •] errorLLMFail ì‚¬ìš© ---
            const message = locales[language]?.['errorLLMFail'] || `Failed to start scenario '${scenarioId}'.`;
             // --- ğŸ‘† [ìˆ˜ì •] ---
            return NextResponse.json({ type: 'error', message: message }, { status: 500 });
        }
    },
};

// determineAction í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ)
async function determineAction(messageText, language = 'ko') {
    if (Object.keys(actionHandlers).includes(messageText)) {
        return { type: messageText };
    }
    const triggeredAction = await findActionByTrigger(messageText);
    if (triggeredAction) {
        if (triggeredAction.type === 'custom') {
            return { type: triggeredAction.value };
        }
        if (triggeredAction.type === 'scenario') {
            return { type: 'START_SCENARIO', payload: { scenarioId: triggeredAction.value } };
        }
    }
    try {
        if (!messageText || typeof messageText !== 'string' || messageText.length > 100) {
             throw new Error("Invalid input, not a scenario ID.");
        }
        await getScenario(messageText);
        return { type: 'START_SCENARIO', payload: { scenarioId: messageText } };
    } catch (e) {
        // console.warn(`Input "${messageText}" is not a valid scenario ID or scenario load failed.`);
    }
    return { type: 'LLM_FALLBACK' };
}


export async function POST(request) {
  let language = 'ko';
  try {
    const body = await request.json();
    language = body.language || language;
    const { message, scenarioState, slots, scenarioSessionId, llmProvider, flowiseApiUrl } = body;

    // 1. ì‹œë‚˜ë¦¬ì˜¤ ì§„í–‰ ì¤‘
    if (scenarioSessionId && scenarioState && scenarioState.scenarioId) {
      const scenario = await getScenario(scenarioState.scenarioId);
      const result = await runScenario(scenario, scenarioState, message, slots, scenarioSessionId, language);
      return NextResponse.json(result);
    }

    // 2. ì‹œë‚˜ë¦¬ì˜¤ ì¬ì‹œì‘
    if (scenarioSessionId && !scenarioState && message && message.text) {
        const scenarioId = message.text;
        const handler = actionHandlers['START_SCENARIO'];
        if (handler) {
            const payload = { scenarioId };
            return await handler(payload, slots || {}, language);
        } else {
             console.error("START_SCENARIO handler not found!");
             throw new Error("Internal server error: Scenario start handler missing.");
        }
    }

    // 3. ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬
    if (!scenarioState && message && message.text) {
        const action = await determineAction(message.text, language);
        const handler = actionHandlers[action.type];

        if (handler) {
            return await handler(action.payload, slots, language);
        }

        if (action.type === 'LLM_FALLBACK') {
            const categories = await getScenarioCategories();
            const allShortcuts = categories.flatMap(cat =>
                cat.subCategories.flatMap(subCat => subCat.items)
            );
            const uniqueShortcuts = [...new Map(allShortcuts.map(item => [item.title, item])).values()];

            const llmResult = await getLlmResponse(message.text, language, uniqueShortcuts, llmProvider, flowiseApiUrl);

            // --- ğŸ‘‡ [ìˆ˜ì •] getLlmResponseê°€ ì—ëŸ¬ ê°ì²´ë¥¼ ë°˜í™˜í•˜ëŠ” ê²½ìš° ì²˜ë¦¬ ---
            if (llmResult instanceof ReadableStream) {
                // ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                return new Response(llmResult, {
                    headers: { 'Content-Type': 'text/event-stream', 'Cache-Control': 'no-cache' },
                });
            } else if (llmResult && llmResult.type === 'error') {
                // getLlmResponseì—ì„œ ë°˜í™˜í•œ í‘œì¤€ ì—ëŸ¬ ê°ì²´
                 console.error('LLM Error from getLlmResponse:', llmResult.message);
                 // í´ë¼ì´ì–¸íŠ¸ì—ê²Œë„ í‘œì¤€ ì—ëŸ¬ ê°ì²´ ì „ë‹¬, ìƒíƒœ ì½”ë“œëŠ” 503 (Service Unavailable) ë˜ëŠ” 500 ì‚¬ìš© ê°€ëŠ¥
                 return NextResponse.json(llmResult, { status: 503 });
            } else if (llmResult && llmResult.response) {
                // ë¹„-ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (Gemini JSON) ë˜ëŠ” ì˜¤ë¥˜ ì‹œ ëŒ€ì²´ ì‘ë‹µ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                return NextResponse.json({
                    type: 'llm_response_with_slots',
                    message: llmResult.response,
                    slots: llmResult.slots || {}, // slotsê°€ ì—†ì„ ê²½ìš° ë¹ˆ ê°ì²´ ë³´ì¥
                });
            } else {
                 // llmResultê°€ ì˜ˆìƒì¹˜ ëª»í•œ í˜•íƒœì¼ ê²½ìš°
                 console.error('Unexpected result from getLlmResponse:', llmResult);
                 throw new Error("Unexpected response format from LLM service.");
            }
            // --- ğŸ‘† [ìˆ˜ì •] ---
        }
    }

    // ëª¨ë“  ì¡°ê±´ ë¶ˆì¼ì¹˜
    console.warn("Chat API received an unhandled request state:", { message, scenarioState, scenarioSessionId });
    return NextResponse.json({ type: 'error', message: locales[language]?.errorUnexpected || 'Invalid request.' }, { status: 400 });

  } catch (error) {
    // --- ğŸ‘‡ [ìˆ˜ì •] ë©”ì¸ catch ë¸”ë¡ì—ì„œ errorLLMFail ì‚¬ìš© ---
    console.error('Chat API Error:', error);
    // getErrorKey ëŒ€ì‹  errorLLMFail ì‚¬ìš© (LLM í˜¸ì¶œ ì‹¤íŒ¨ ì™¸ ë‹¤ë¥¸ ì˜¤ë¥˜ë„ ì´ ë©”ì‹œì§€ë¡œ í†µì¼)
    const message = locales[language]?.['errorLLMFail'] || 'An unexpected error occurred. Please try again later.';

    return NextResponse.json(
      { type: 'error', message: message },
      { status: 500 } // ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜
    );
    // --- ğŸ‘† [ìˆ˜ì •] ---
  }
}