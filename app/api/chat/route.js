import { NextResponse } from 'next/server';
import { getScenario, getNextNode, interpolateMessage, findScenarioIdByTrigger, getScenarioList, validateInput, getNestedValue } from '../../lib/chatbotEngine';
import { getGeminiStream } from '../../lib/gemini';

async function handleScenario(scenario, scenarioState, message, slots) {
    const { scenarioId, currentNodeId, awaitingInput } = scenarioState;
    let currentId = currentNodeId;
    let newSlots = { ...slots };

    if (awaitingInput) {
        const currentNode = scenario.nodes.find(n => n.id === currentId);
        const validation = currentNode.data.validation;
        const { isValid, message: validationMessage } = validateInput(message.text, validation);

        if (!isValid) {
            return NextResponse.json({
                type: 'scenario_validation_fail',
                message: validationMessage,
                scenarioState: { ...scenarioState, awaitingInput: true }, // Awaiting input again
                slots: newSlots,
            });
        }
        newSlots[currentNode.data.slot] = message.text;
    }

    let nextNode;
    if (awaitingInput) {
         // Input was valid, proceed from the current node
         nextNode = getNextNode(scenario, currentId, message.sourceHandle, newSlots);
    } else {
        // This is not a response to a slot-filling request, so get the next node based on the handle
        nextNode = getNextNode(scenario, currentId, message.sourceHandle, newSlots);
    }

    // Process nodes until an interactive one is found
    while (nextNode) {
        const interpolatedContent = interpolateMessage(nextNode.data.content, newSlots);
        nextNode.data.content = interpolatedContent;

        if (nextNode.type === 'slotfilling') {
            return NextResponse.json({
                type: 'scenario',
                nextNode,
                scenarioState: { scenarioId, currentNodeId: nextNode.id, awaitingInput: true },
                slots: newSlots,
            });
        }

        if (nextNode.type === 'api') {
            const { method, url, headers, body, responseMapping } = nextNode.data;
            const interpolatedUrl = interpolateMessage(url, newSlots);
            const interpolatedHeaders = JSON.parse(interpolateMessage(headers || '{}', newSlots));
            const interpolatedBody = method !== 'GET' && body ? interpolateMessage(body, newSlots) : undefined;

            let isSuccess = false;
            try {
                const response = await fetch(interpolatedUrl, { method, headers: interpolatedHeaders, body: interpolatedBody });
                if (!response.ok) throw new Error(`API request failed with status ${response.status}`);
                
                const result = await response.json();
                
                if (responseMapping && responseMapping.length > 0) {
                    responseMapping.forEach(mapping => {
                        const value = getNestedValue(result, mapping.path);
                        if (value !== undefined) newSlots[mapping.slot] = value;
                    });
                }
                isSuccess = true;
            } catch (error) {
                console.error("API Node Error:", error);
                isSuccess = false;
            }
            
            nextNode = getNextNode(scenario, nextNode.id, isSuccess ? 'onSuccess' : 'onError', newSlots);
            continue; // Continue processing with the next node
        }

        if (nextNode.type === 'llm') {
             const interpolatedPrompt = interpolateMessage(nextNode.data.prompt, newSlots);
             const stream = await getGeminiStream(interpolatedPrompt);
             const reader = stream.getReader();
             const decoder = new TextDecoder();
             let llmResponse = '';
             while (true) {
                 const { value, done } = await reader.read();
                 if (done) break;
                 llmResponse += decoder.decode(value, { stream: true });
             }

             if (nextNode.data.outputVar) {
                 newSlots[nextNode.data.outputVar] = llmResponse;
             }

             nextNode = getNextNode(scenario, nextNode.id, null, newSlots);
             continue;
        }

        if (nextNode.type === 'message' || nextNode.type === 'branch' || nextNode.type === 'form') {
           // These are interactive nodes that require user response
           break;
        }
        
        // For non-interactive nodes like 'message' without replies, just move to the next one
        const nonInteractiveNext = getNextNode(scenario, nextNode.id, null, newSlots);
        if(!nonInteractiveNext) break; // End of path
        
        // If the message node has no replies and an edge exists, proceed automatically
        if (nextNode.type === 'message' && (!nextNode.data.replies || nextNode.data.replies.length === 0)) {
            currentId = nextNode.id; // Update currentId to the message node we just processed
            nextNode = getNextNode(scenario, currentId, null, newSlots);
        } else {
             // It's an interactive node, so we break to send it to the user
            break;
        }
    }

    if (nextNode) {
        return NextResponse.json({
            type: 'scenario',
            nextNode,
            scenarioState: { scenarioId, currentNodeId: nextNode.id, awaitingInput: false },
            slots: newSlots,
        });
    } else {
        return NextResponse.json({
            type: 'scenario_end',
            message: 'ì‹œë‚˜ë¦¬ì˜¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
            scenarioState: null,
            slots: newSlots,
        });
    }
}

// --- ğŸ‘‡ [ì¶”ê°€ëœ ë¶€ë¶„] ---

/**
 * ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰í•  ì‘ì—…ì„ ê²°ì •í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
 * @param {string} messageText - ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
 * @returns {Promise<{type: string, payload?: any}>} - ì‘ì—… ìœ í˜•ê³¼ í•„ìš”í•œ ë°ì´í„°ë¥¼ ë‹´ì€ ê°ì²´
 */
async function determineAction(messageText) {
    // 1. í‚¤ì›Œë“œ ê¸°ë°˜ íŠ¸ë¦¬ê±° í™•ì¸
    const triggeredAction = findScenarioIdByTrigger(messageText);
    if (triggeredAction) {
        return { type: triggeredAction };
    }

    // 2. ë©”ì‹œì§€ ìì²´ê°€ ì‹œë‚˜ë¦¬ì˜¤ IDì¸ì§€ í™•ì¸
    try {
        await getScenario(messageText);
        // getScenarioê°€ ì—ëŸ¬ë¥¼ ë˜ì§€ì§€ ì•Šìœ¼ë©´ í•´ë‹¹ IDì˜ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì¡´ì¬í•¨
        return { type: 'START_SCENARIO', payload: { scenarioId: messageText } };
    } catch (e) {
        // ì‹œë‚˜ë¦¬ì˜¤ ì—†ìŒ, ë¬´ì‹œí•˜ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰
    }

    // 3. ìœ„ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ LLM í˜¸ì¶œ
    return { type: 'LLM_FALLBACK' };
}

// ê° ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ ë§µ
const actionHandlers = {
    'GET_SCENARIO_LIST': async () => {
        const scenarios = await getScenarioList();
        return NextResponse.json({
            type: 'scenario_list',
            scenarios,
            message: 'ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.'
        });
    },
    'START_SCENARIO': async (payload, slots) => {
        const { scenarioId } = payload;
        const scenario = await getScenario(scenarioId);
        const startNode = getNextNode(scenario, null, null);

        const interpolatedContent = interpolateMessage(startNode.data.content, slots);
        startNode.data.content = interpolatedContent;

        return NextResponse.json({
            type: 'scenario_start',
            nextNode: startNode,
            scenarioState: { scenarioId: scenarioId, currentNodeId: startNode.id, awaitingInput: false },
            slots: {}
        });
    },
    // í‚¤ì›Œë“œ íŠ¸ë¦¬ê±°ë¡œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œì‘í•˜ëŠ” ê²½ìš° (ì˜ˆ: "ì˜ˆì•½")
    'reservation-scenario': (payload, slots) => actionHandlers.START_SCENARIO({ scenarioId: 'reservation-scenario' }, slots),
    'faq-scenario': (payload, slots) => actionHandlers.START_SCENARIO({ scenarioId: 'faq-scenario' }, slots),
    'Welcome': (payload, slots) => actionHandlers.START_SCENARIO({ scenarioId: 'Welcome' }, slots),
};
// --- ğŸ‘† [ì—¬ê¸°ê¹Œì§€] ---


export async function POST(request) {
  try {
    const body = await request.json();
    const { message, scenarioState, slots } = body;
    
    // 1. ì‹œë‚˜ë¦¬ì˜¤ê°€ ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ ê²½ìš° ìš°ì„  ì²˜ë¦¬
    if (scenarioState && scenarioState.scenarioId) {
      const scenario = await getScenario(scenarioState.scenarioId);
      return await handleScenario(scenario, scenarioState, message, slots);
    }
    
    // --- ğŸ‘‡ [ìˆ˜ì •ëœ ë¶€ë¶„] ---

    // 2. ìƒˆë¡œìš´ ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ì—… ê²°ì •
    const action = await determineAction(message.text);
    const handler = actionHandlers[action.type];

    if (handler) {
        // ê²°ì •ëœ ì‘ì—…ì— ë§ëŠ” í•¸ë“¤ëŸ¬ ì‹¤í–‰
        return await handler(action.payload, slots);
    }

    // 3. ì§€ì •ëœ ì‘ì—…ì´ ì—†ëŠ” ê²½ìš°, ê¸°ë³¸ Gemini API í˜¸ì¶œ (LLM_FALLBACK)
    const stream = await getGeminiStream(message.text);
    return new Response(stream, {
      headers: { 'Content-Type': 'text/plain; charset=utf-8' },
    });

    // --- ğŸ‘† [ì—¬ê¸°ê¹Œì§€] ---

  } catch (error) {
    console.error('Chat API Error:', error);
    return NextResponse.json(
      { error: error.message || 'An error occurred.' },
      { status: 500 }
    );
  }
}