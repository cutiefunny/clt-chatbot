import { NextResponse } from 'next/server';
import { getScenario, getNextNode, interpolateMessage, findScenarioIdByTrigger, getScenarioList, runScenario } from '../../lib/chatbotEngine';
import { getGeminiStream } from '../../lib/gemini';
import { locales } from '../../lib/locales';

async function determineAction(messageText) {
    const triggeredAction = findScenarioIdByTrigger(messageText);
    if (triggeredAction) {
        return { type: triggeredAction };
    }

    try {
        await getScenario(messageText);
        return { type: 'START_SCENARIO', payload: { scenarioId: messageText } };
    } catch (e) {
        // Scenario not found, proceed to LLM
    }

    return { type: 'LLM_FALLBACK' };
}

const actionHandlers = {
    'GET_SCENARIO_LIST': async (payload, slots, language) => {
        const scenarios = await getScenarioList();
        return NextResponse.json({
            type: 'scenario_list',
            scenarios,
            message: locales[language].scenarioListMessage,
            scenarioState: null
        });
    },
    'START_SCENARIO': async (payload, slots) => {
        const { scenarioId } = payload;
        const scenario = await getScenario(scenarioId);
        const startNode = getNextNode(scenario, null, null);

        if (!startNode || !startNode.data) {
            return NextResponse.json({
                type: 'scenario_end',
                message: `ì‹œë‚˜ë¦¬ì˜¤ '${scenarioId}'ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë‚´ìš©ì´ ë¹„ì–´ìˆê±°ë‚˜ ì‹œì‘ì ì´ ì—†ìŠµë‹ˆë‹¤.)`,
                scenarioState: null,
                slots: {}
            });
        }

        if (startNode.data.content) {
            const interpolatedContent = interpolateMessage(startNode.data.content, slots);
            startNode.data.content = interpolatedContent;
        }

        return NextResponse.json({
            type: 'scenario_start',
            nextNode: startNode,
            scenarioState: { scenarioId: scenarioId, currentNodeId: startNode.id, awaitingInput: false },
            slots: {}
        });
    },
    'ì„ ë°• ì˜ˆì•½': (payload, slots) => actionHandlers.START_SCENARIO({ scenarioId: 'ì„ ë°• ì˜ˆì•½' }, slots),
    'faq-scenario': (payload, slots) => actionHandlers.START_SCENARIO({ scenarioId: 'faq-scenario' }, slots),
    'Welcome': (payload, slots) => actionHandlers.START_SCENARIO({ scenarioId: 'Welcome' }, slots),
};


export async function POST(request) {
  try {
    const body = await request.json();
    const { message, scenarioState, slots, language = 'ko', scenarioSessionId } = body;

    // Case 1: Continue existing scenario
    if (scenarioSessionId && scenarioState && scenarioState.scenarioId) {
      const scenario = await getScenario(scenarioState.scenarioId);
      const result = await runScenario(scenario, scenarioState, message, slots, scenarioSessionId);
      return NextResponse.json(result);
    }
    
    // --- ğŸ‘‡ [ìˆ˜ì •ëœ ë¶€ë¶„] ---
    // Case 2: Start a new scenario for a pre-created session
    if (scenarioSessionId && !scenarioState && message && message.text) {
        const scenarioId = message.text;
        const handler = actionHandlers['START_SCENARIO'];
        const payload = { scenarioId };
        return await handler(payload, slots || {}, language);
    }
    // --- ğŸ‘† [ì—¬ê¸°ê¹Œì§€] ---

    // Case 3: A regular message from user, determine what to do
    if (!scenarioState && message.text) {
        const action = await determineAction(message.text);
        const handler = actionHandlers[action.type];

        if (handler) {
            return await handler(action.payload, slots, language);
        }
    }

    // Fallback to LLM
    const stream = await getGeminiStream(message.text, language);
    return new Response(stream, {
      headers: { 'Content-Type': 'text/plain; charset=utf-8' },
    });

  } catch (error) {
    console.error('Chat API Error:', error);
    return NextResponse.json(
      { error: error.message || 'An error occurred.' },
      { status: 500 }
    );
  }
}